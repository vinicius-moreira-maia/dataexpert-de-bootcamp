{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94f23c25-2d93-4984-81b3-4486576cf955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/07 00:38:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast, split, lit\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "matches_bucketed = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/iceberg/data/matches.csv\")\n",
    "\n",
    "match_details_bucketed = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/iceberg/data/match_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27fcf39-e8a7-4b01-8949-db91326e2859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches_bucketed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b72756d-6892-4b8c-8071-732594adbdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_details_bucketed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821ac8cc-20cb-43d1-a5a9-23a363ddab5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS bootcamp.matches_bucketed\")\n",
    "\n",
    "# particionamento das duas tabelas iceberg em 16 buckets, usando hashes da coluna match_id\n",
    "# isso torna o join das duas tabelas bastante eficiente, pois os buckets/partições irão se alinhar\n",
    "# ou seja, o bucket 0 da primeira tabela irá se alinhar com o bucket 0 da segunda tabela\n",
    "# \"sempre usar potências de 2\"\n",
    "\n",
    "# o nº de arquivos/partições produzidas aqui será a quantidade de datas únicas (completion_date) X 16 (nº de buckets)\n",
    "\n",
    "'''\n",
    "bucketed_matches_ddl = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.matches_bucketed (\n",
    "    match_id STRING,\n",
    "    is_team_game BOOLEAN,\n",
    "    playlist_id STRING,\n",
    "    completion_date TIMESTAMP\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (completion_date, bucket(16, match_id))\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(bucketed_matches_ddl)\n",
    "\n",
    "# o nº de arquivos/partições produzidas aqui será 16 (nº de buckets)\n",
    "bucketed_details_ddl = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.match_details_bucketed (\n",
    "    match_id STRING,\n",
    "    player_gamertag STRING,\n",
    "    player_total_kills INT,\n",
    "    player_total_deaths INT\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (bucket(16, match_id))\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(bucketed_details_ddl)\n",
    "'''\n",
    "\n",
    "# tabelas iceberg não suportam buckets\n",
    "\n",
    "bucketed_matches_ddl = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.matches_bucketed (\n",
    "    match_id STRING,\n",
    "    is_team_game BOOLEAN,\n",
    "    playlist_id STRING,\n",
    "    completion_date TIMESTAMP\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (completion_date)\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(bucketed_matches_ddl)\n",
    "\n",
    "bucketed_details_ddl = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.match_details_bucketed (\n",
    "    match_id STRING,\n",
    "    player_gamertag STRING,\n",
    "    player_total_kills INT,\n",
    "    player_total_deaths INT\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(bucketed_details_ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a663d6-6486-48b5-8095-4abafec369d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# tá dando OOM =(\n",
    "\n",
    "'''\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "matches_bucketed.select( \"match_id\", \"is_team_game\", \"playlist_id\", \"completion_date\") \\\n",
    "  .write.mode(\"append\") \\\n",
    "  .bucketBy(16, \"match_id\") \\\n",
    "  .partitionBy(\"completion_date\") \\\n",
    "  .saveAsTable(\"bootcamp.matches_bucketed\")\n",
    "\n",
    "match_details_bucketed.select( \"match_id\", \"player_gamertag\", \"player_total_kills\", \"player_total_deaths\") \\\n",
    "  .write.mode(\"append\") \\\n",
    "  .bucketBy(16, \"match_id\") \\\n",
    "  .saveAsTable(\"bootcamp.match_details_bucketed\")\n",
    "'''\n",
    "\n",
    "# forma correta de escrever dados em tabelas Iceberg (não há bucket join!!)\n",
    "matches_bucketed.select( \"match_id\", \"is_team_game\", \"playlist_id\", \"completion_date\") \\\n",
    "  .writeTo(\"bootcamp.matches_bucketed\").append()\n",
    "\n",
    "match_details_bucketed.select( \"match_id\", \"player_gamertag\", \"player_total_kills\", \"player_total_deaths\") \\\n",
    "  .writeTo(\"bootcamp.match_details_bucketed\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37892045-af76-4874-9ed1-cb74ad239178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evitando broadcast join automático\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e37169b-1f2e-4f17-9b89-fb2ac3ee8712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 20:30:22 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "matches_bucketed.createOrReplaceTempView(\"matches\")\n",
    "match_details_bucketed.createOrReplaceTempView(\"match_details\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c49e225e-7dfc-4de6-8f6a-420dd5b5da28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [match_id#245], [match_id#249], Inner\n",
      "   :- Sort [match_id#245 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(match_id#245, 200), ENSURE_REQUIREMENTS, [plan_id=159]\n",
      "   :     +- BatchScan demo.bootcamp.match_details_bucketed[match_id#245, player_gamertag#246, player_total_kills#247, player_total_deaths#248] demo.bootcamp.match_details_bucketed (branch=null) [filters=match_id IS NOT NULL, groupedBy=] RuntimeFilters: []\n",
      "   +- Sort [match_id#249 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(match_id#249, 200), ENSURE_REQUIREMENTS, [plan_id=160]\n",
      "         +- Filter isnotnull(match_id#249)\n",
      "            +- BatchScan demo.bootcamp.matches_bucketed[match_id#249, is_team_game#250, playlist_id#251, completion_date#252] demo.bootcamp.matches_bucketed (branch=null) [filters=match_id IS NOT NULL, groupedBy=] RuntimeFilters: []\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [match_id#54], [match_id#17], Inner\n",
      "   :- Sort [match_id#54 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(match_id#54, 200), ENSURE_REQUIREMENTS, [plan_id=186]\n",
      "   :     +- Filter isnotnull(match_id#54)\n",
      "   :        +- FileScan csv [match_id#54,player_gamertag#55,previous_spartan_rank#56,spartan_rank#57,previous_total_xp#58,total_xp#59,previous_csr_tier#60,previous_csr_designation#61,previous_csr#62,previous_csr_percent_to_next_tier#63,previous_csr_rank#64,current_csr_tier#65,current_csr_designation#66,current_csr#67,current_csr_percent_to_next_tier#68,current_csr_rank#69,player_rank_on_team#70,player_finished#71,player_average_life#72,player_total_kills#73,player_total_headshots#74,player_total_weapon_damage#75,player_total_shots_landed#76,player_total_melee_kills#77,... 12 more fields] Batched: false, DataFilters: [isnotnull(match_id#54)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/match_details.csv], PartitionFilters: [], PushedFilters: [IsNotNull(match_id)], ReadSchema: struct<match_id:string,player_gamertag:string,previous_spartan_rank:int,spartan_rank:int,previous...\n",
      "   +- Sort [match_id#17 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(match_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=187]\n",
      "         +- Filter isnotnull(match_id#17)\n",
      "            +- FileScan csv [match_id#17,mapid#18,is_team_game#19,playlist_id#20,game_variant_id#21,is_match_over#22,completion_date#23,match_duration#24,game_mode#25,map_variant_id#26] Batched: false, DataFilters: [isnotnull(match_id#17)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/matches.csv], PartitionFilters: [], PushedFilters: [IsNotNull(match_id)], ReadSchema: struct<match_id:string,mapid:string,is_team_game:boolean,playlist_id:string,game_variant_id:strin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pelo fato de ter dado OOM na inserção dos dados eu não consigo ver muito bem o resultado da bucketização no join\n",
    "# Mas mesmo assim o plano de execução do primeiro Join ainda é menor\n",
    "\n",
    "# Como o do Zach rodou a inserção, não teve shuffle nenhum nesse primeiro join\n",
    "\n",
    "# join entre as tabelas \"bucketizadas\"\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM bootcamp.match_details_bucketed mdb \n",
    "    JOIN bootcamp.matches_bucketed md \n",
    "    ON mdb.match_id = md.match_id\n",
    "\"\"\").explain()\n",
    "\n",
    "# join entre as views em cache\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM match_details mdb \n",
    "    JOIN matches md \n",
    "    ON mdb.match_id = md.match_id\n",
    "\"\"\").explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51f8ba8a-7919-42ca-acad-8b6d107de00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7817a5-81bc-4bf4-9d9a-888c7349aa52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
