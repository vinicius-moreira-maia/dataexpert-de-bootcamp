{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c455765-0c05-4609-b54f-70c946da1bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/07 15:11:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/08/07 15:11:17 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/08/07 15:11:17 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast, split, lit, expr\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# desabilitando broadcast join automático\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "'''\n",
    "CONFIGURAÇÕES DE STORAGE PARTITION JOIN (SPJ)\n",
    "\n",
    "SPJ é uma otimização de join no Spark SQL que evita o shuffle (embaralhamento de dados entre nós) \n",
    "ao aproveitar o layout físico dos dados já particionados no armazenamento — como S3, HDFS ou MinIO.\n",
    "\n",
    "SPJ é uma generalização do Bucket Join, suportando muito mais casos, especialmente em formatos \n",
    "modernos como Iceberg (fontes de dados V2).\n",
    "'''\n",
    "\n",
    "# tenta eliminar shuffle usando o particionamento da própria fonte de dados\n",
    "spark.conf.set(\"spark.sql.sources.v2.bucketing.enabled\", \"true\")\n",
    "\n",
    "# força o planner do Spark a preservar o agrupamento físico dos dados ao ler \n",
    "# tabelas Iceberg — principalmente quando elas são bucketed ou partitioned\n",
    "spark.conf.set(\"spark.sql.iceberg.planning.preserve-data-grouping\", \"true\")\n",
    "\n",
    "# tenta eliminar shuffle quando um lado do join não possui valores das partições do outro lado\n",
    "# requer que 'spark.sql.sources.v2.bucketing.enabled' seja true \n",
    "spark.conf.set(\"spark.sql.sources.v2.bucketing.pushPartValues.enabled\", \"true\")\n",
    "\n",
    "# requer que as chaves do join ou merge sejam as mesmas e estejam na mesma ordem para eliminar shuffle\n",
    "# setando para false eu flexibilizo isso\n",
    "spark.conf.set(\"spark.sql.requireAllClusterKeysForCoPartition\", \"false\")\n",
    "\n",
    "# isso aqui aparentemente otimiza o join  quando há desbalanceamento na distribuição dos dados\n",
    "# requer que 'spark.sql.sources.v2.bucketing.enabled' seja true\n",
    "# requer que 'spark.sql.sources.v2.bucketing.pushPartValues.enabled' seja true\n",
    "spark.conf.set(\"spark.sql.sources.v2.bucketing.partiallyClusteredDistribution.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bad47ec4-ea09-4c09-a6d5-240c7a68aa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "matches = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/iceberg/data/matches.csv\")\n",
    "\n",
    "match_details = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/iceberg/data/match_details.csv\")\n",
    "\n",
    "medals_matches_players = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/iceberg/data/medals_matches_players.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797e4e62-eb4a-4dbf-b643-5370c192d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Bucket join `match_details`, `matches`, and `medal_matches_players` on `match_id` with `16` buckets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18aaf637-993a-42e3-a12d-6e2118a8a730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmatches.writeTo(\"bootcamp.matches_bucket_table\")   .using(\"iceberg\")   .partitionedBy(expr(\"bucket(16, match_id)\"))   .createOrReplace()\\n\\nmatch_details.writeTo(\"bootcamp.match_details_bucket_table\")   .using(\"iceberg\")   .partitionedBy(expr(\"bucket(16, match_id)\"))   .createOrReplace()\\n\\nmedals_matches_players.writeTo(\"bootcamp.medals_matches_players_bucket_table\")   .using(\"iceberg\")   .partitionedBy(expr(\"bucket(16, match_id)\"))   .createOrReplace()\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertendo os csv's para parquet's \"bucketizados\"\n",
    "\n",
    "# Cria o database caso não exista\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bootcamp\")\n",
    "\n",
    "# o correto aqui seria eu escrever tabelas iceberg no lugar de parquet's\n",
    "# mas por algum motivo ainda assim ele faz bucket join/storage partition join\n",
    "\n",
    "matches.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .bucketBy(16, \"match_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"bootcamp.matches_bucket\")\n",
    "\n",
    "match_details.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .bucketBy(16, \"match_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"bootcamp.match_details_bucket\")\n",
    "\n",
    "medals_matches_players.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .bucketBy(16, \"match_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"bootcamp.medals_matches_players_bucket\")\n",
    "\n",
    "'''\n",
    "matches.writeTo(\"bootcamp.matches_bucket_table\") \\\n",
    "  .using(\"iceberg\") \\\n",
    "  .partitionedBy(expr(\"bucket(16, match_id)\")) \\\n",
    "  .createOrReplace()\n",
    "\n",
    "match_details.writeTo(\"bootcamp.match_details_bucket_table\") \\\n",
    "  .using(\"iceberg\") \\\n",
    "  .partitionedBy(expr(\"bucket(16, match_id)\")) \\\n",
    "  .createOrReplace()\n",
    "\n",
    "medals_matches_players.writeTo(\"bootcamp.medals_matches_players_bucket_table\") \\\n",
    "  .using(\"iceberg\") \\\n",
    "  .partitionedBy(expr(\"bucket(16, match_id)\")) \\\n",
    "  .createOrReplace()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1a8978b-6bb3-42eb-ab01-e956701b0188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lendo os parquet's 'bucketizados'\n",
    "matches_bucketed = spark.table(\"bootcamp.matches_bucket\")\n",
    "match_details_bucketed = spark.table(\"bootcamp.match_details_bucket\")\n",
    "medals_matches_players_bucketed = spark.table(\"bootcamp.medals_matches_players_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ab6030c-e9d8-4239-8ee6-98b18da42d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [match_id#1894, mapid#1895, is_team_game#1896, playlist_id#1897, game_variant_id#1898, is_match_over#1899, completion_date#1900, match_duration#1901, game_mode#1902, map_variant_id#1903, player_gamertag#1915, previous_spartan_rank#1916, spartan_rank#1917, previous_total_xp#1918, total_xp#1919, previous_csr_tier#1920, previous_csr_designation#1921, previous_csr#1922, previous_csr_percent_to_next_tier#1923, previous_csr_rank#1924, current_csr_tier#1925, current_csr_designation#1926, current_csr#1927, current_csr_percent_to_next_tier#1928, ... 24 more fields]\n",
      "   +- SortMergeJoin [match_id#1894], [match_id#1986], Inner\n",
      "      :- Project [match_id#1894, mapid#1895, is_team_game#1896, playlist_id#1897, game_variant_id#1898, is_match_over#1899, completion_date#1900, match_duration#1901, game_mode#1902, map_variant_id#1903, player_gamertag#1915, previous_spartan_rank#1916, spartan_rank#1917, previous_total_xp#1918, total_xp#1919, previous_csr_tier#1920, previous_csr_designation#1921, previous_csr#1922, previous_csr_percent_to_next_tier#1923, previous_csr_rank#1924, current_csr_tier#1925, current_csr_designation#1926, current_csr#1927, current_csr_percent_to_next_tier#1928, ... 21 more fields]\n",
      "      :  +- SortMergeJoin [match_id#1894], [match_id#1914], Inner\n",
      "      :     :- Sort [match_id#1894 ASC NULLS FIRST], false, 0\n",
      "      :     :  +- BatchScan demo.bootcamp.matches_bucket[match_id#1894, mapid#1895, is_team_game#1896, playlist_id#1897, game_variant_id#1898, is_match_over#1899, completion_date#1900, match_duration#1901, game_mode#1902, map_variant_id#1903] demo.bootcamp.matches_bucket (branch=null) [filters=match_id IS NOT NULL, groupedBy=match_id_bucket] RuntimeFilters: []\n",
      "      :     +- Sort [match_id#1914 ASC NULLS FIRST], false, 0\n",
      "      :        +- BatchScan demo.bootcamp.match_details_bucket[match_id#1914, player_gamertag#1915, previous_spartan_rank#1916, spartan_rank#1917, previous_total_xp#1918, total_xp#1919, previous_csr_tier#1920, previous_csr_designation#1921, previous_csr#1922, previous_csr_percent_to_next_tier#1923, previous_csr_rank#1924, current_csr_tier#1925, current_csr_designation#1926, current_csr#1927, current_csr_percent_to_next_tier#1928, current_csr_rank#1929, player_rank_on_team#1930, player_finished#1931, player_average_life#1932, player_total_kills#1933, player_total_headshots#1934, player_total_weapon_damage#1935, player_total_shots_landed#1936, player_total_melee_kills#1937, ... 12 more fields] demo.bootcamp.match_details_bucket (branch=null) [filters=match_id IS NOT NULL, groupedBy=match_id_bucket] RuntimeFilters: []\n",
      "      +- Sort [match_id#1986 ASC NULLS FIRST], false, 0\n",
      "         +- BatchScan demo.bootcamp.medals_matches_players_bucket[match_id#1986, player_gamertag#1987, medal_id#1988L, count#1989] demo.bootcamp.medals_matches_players_bucket (branch=null) [filters=match_id IS NOT NULL, groupedBy=match_id_bucket] RuntimeFilters: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sem shuffle!! =)\n",
    "matches_bucketed.join(match_details_bucketed, on=\"match_id\") \\\n",
    "                .join(medals_matches_players_bucketed, on=\"match_id\") \\\n",
    "                .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "def646e6-59d5-441d-9b9e-bf1d2c7c6e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = matches_bucketed.join(match_details_bucketed, on=\"match_id\") \\\n",
    "                .join(medals_matches_players_bucketed, on=\"match_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f22f393-4e04-46e4-b19a-ee0f7278e143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- match_id: string (nullable = true)\n",
      " |-- mapid: string (nullable = true)\n",
      " |-- is_team_game: boolean (nullable = true)\n",
      " |-- playlist_id: string (nullable = true)\n",
      " |-- game_variant_id: string (nullable = true)\n",
      " |-- is_match_over: boolean (nullable = true)\n",
      " |-- completion_date: timestamp (nullable = true)\n",
      " |-- match_duration: string (nullable = true)\n",
      " |-- game_mode: string (nullable = true)\n",
      " |-- map_variant_id: string (nullable = true)\n",
      " |-- player_gamertag: string (nullable = true)\n",
      " |-- previous_spartan_rank: integer (nullable = true)\n",
      " |-- spartan_rank: integer (nullable = true)\n",
      " |-- previous_total_xp: integer (nullable = true)\n",
      " |-- total_xp: integer (nullable = true)\n",
      " |-- previous_csr_tier: integer (nullable = true)\n",
      " |-- previous_csr_designation: integer (nullable = true)\n",
      " |-- previous_csr: integer (nullable = true)\n",
      " |-- previous_csr_percent_to_next_tier: integer (nullable = true)\n",
      " |-- previous_csr_rank: integer (nullable = true)\n",
      " |-- current_csr_tier: integer (nullable = true)\n",
      " |-- current_csr_designation: integer (nullable = true)\n",
      " |-- current_csr: integer (nullable = true)\n",
      " |-- current_csr_percent_to_next_tier: integer (nullable = true)\n",
      " |-- current_csr_rank: integer (nullable = true)\n",
      " |-- player_rank_on_team: integer (nullable = true)\n",
      " |-- player_finished: boolean (nullable = true)\n",
      " |-- player_average_life: string (nullable = true)\n",
      " |-- player_total_kills: integer (nullable = true)\n",
      " |-- player_total_headshots: integer (nullable = true)\n",
      " |-- player_total_weapon_damage: double (nullable = true)\n",
      " |-- player_total_shots_landed: integer (nullable = true)\n",
      " |-- player_total_melee_kills: integer (nullable = true)\n",
      " |-- player_total_melee_damage: double (nullable = true)\n",
      " |-- player_total_assassinations: integer (nullable = true)\n",
      " |-- player_total_ground_pound_kills: integer (nullable = true)\n",
      " |-- player_total_shoulder_bash_kills: integer (nullable = true)\n",
      " |-- player_total_grenade_damage: double (nullable = true)\n",
      " |-- player_total_power_weapon_damage: double (nullable = true)\n",
      " |-- player_total_power_weapon_grabs: integer (nullable = true)\n",
      " |-- player_total_deaths: integer (nullable = true)\n",
      " |-- player_total_assists: integer (nullable = true)\n",
      " |-- player_total_grenade_kills: integer (nullable = true)\n",
      " |-- did_win: integer (nullable = true)\n",
      " |-- team_id: integer (nullable = true)\n",
      " |-- player_gamertag: string (nullable = true)\n",
      " |-- medal_id: long (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11409f08-d021-47b4-b973-127f49ace678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:===================================>                    (10 + 6) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+\n",
      "|player_gamertag|avg_kills|\n",
      "+---------------+---------+\n",
      "|   gimpinator14|    109.0|\n",
      "+---------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Which player averages the most kills per game?\n",
    "\n",
    "final_df.select(\"match_details_bucket.match_id\", \n",
    "                \"match_details_bucket.player_gamertag\", \n",
    "                \"match_details_bucket.player_total_kills\") \\\n",
    "        .createOrReplaceTempView(\"final_df_1\")\n",
    "\n",
    "sql_exp = '''\n",
    "-- essa cte garante apenas 1 linha de jogador por partida\n",
    "WITH dedup AS (\n",
    "  SELECT *,\n",
    "         ROW_NUMBER() OVER (PARTITION BY match_id, player_gamertag ORDER BY match_id) AS rn\n",
    "  FROM final_df_1\n",
    ")\n",
    "SELECT player_gamertag, AVG(player_total_kills) AS avg_kills\n",
    "FROM dedup\n",
    "WHERE rn = 1\n",
    "GROUP BY player_gamertag\n",
    "ORDER BY 2 DESC\n",
    "'''\n",
    "\n",
    "res = spark.sql(sql_exp)\n",
    "\n",
    "res.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fa15da0e-c236-4832-a38b-ef93e4a4ba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:============================>                            (8 + 8) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|         playlist_id|sum_playlist_played|\n",
      "+--------------------+-------------------+\n",
      "|f72e0ef0-7c4a-430...|               7640|\n",
      "+--------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Which playlist gets played the most?\n",
    "\n",
    "final_df.select(\"matches_bucket.match_id\", \n",
    "                \"matches_bucket.playlist_id\") \\\n",
    "        .createOrReplaceTempView(\"final_df_2\")\n",
    "\n",
    "sql_exp = '''\n",
    "-- essa cte garante apenas 1 linha de playlist por partida\n",
    "WITH dedup AS (\n",
    "  SELECT *,\n",
    "         ROW_NUMBER() OVER (PARTITION BY match_id, playlist_id ORDER BY match_id) AS rn\n",
    "  FROM final_df_2\n",
    ")\n",
    "SELECT playlist_id, COUNT(1) AS sum_playlist_played -- COUNT ou SUM, nesse caso tanto faz\n",
    "FROM dedup\n",
    "WHERE rn = 1\n",
    "GROUP BY playlist_id\n",
    "ORDER BY 2 DESC\n",
    "'''\n",
    "\n",
    "res = spark.sql(sql_exp)\n",
    "\n",
    "res.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1c887a61-c7fa-4d84-8cb7-6d6aae92468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
