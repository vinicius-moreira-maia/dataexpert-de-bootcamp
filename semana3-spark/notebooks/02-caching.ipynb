{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80e0cba-dd05-4954-a95a-27388b756395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 19:05:44 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# importanto funções de agregação comuns\n",
    "from pyspark.sql.functions import col, count, collect_list, max\n",
    "\n",
    "# nível de armazenamento em memória (MEMORY_ONLY, DISK_ONLY...)\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# sessão Spark é o ponto de entrada para usar DataFrames\n",
    "spark = SparkSession.builder.appName(\"Bootcamp PySpark\").getOrCreate()\n",
    "\n",
    "# desativando o broadcast join automático (o spark faz automaticamente para datasets de até 10 MB)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "# permite self-joins entre dataframes sem a necessidade de criar alias para evitar ambiguidade\n",
    "# quando isso ocorrer, não vai falhar\n",
    "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")\n",
    "\n",
    "# alterando as partições produzidas por shuffle's de 200 (default) para 4\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9af03438-68f9-472b-8959-d88ac39cc775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# views temporárias ficam disponíveis apenas durante a sessão\n",
    "# podem ser consultadas tranquilamente via sql\n",
    "\n",
    "users = spark.read \\\n",
    "             .option(\"header\", \"true\") \\\n",
    "             .option(\"inferSchema\", \"true\") \\\n",
    "             .csv(\"/home/iceberg/data/events.csv\") \\\n",
    "             .where(col(\"user_id\").isNotNull())\n",
    "\n",
    "users.createOrReplaceTempView(\"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "589ba4f8-b238-42ab-88c8-d2f4626677bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------+--------------------+---+--------------------+\n",
      "|    user_id|device_id|referrer|                host|url|          event_time|\n",
      "+-----------+---------+--------+--------------------+---+--------------------+\n",
      "| 1037710827|532630305|    NULL| www.zachwilson.tech|  /|2021-03-08 17:27:...|\n",
      "|  925588856|532630305|    NULL|    www.eczachly.com|  /|2021-05-10 11:26:...|\n",
      "|-1180485268|532630305|    NULL|admin.zachwilson....|  /|2021-02-17 16:19:...|\n",
      "+-----------+---------+--------+--------------------+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcf10d43-4dda-4401-b591-3a76b0f9f5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache de um DataFrame com agregações por usuário e device\n",
    "# cache deve ser usado com cuidado (< 5GB) / ou para broadcast join\n",
    "# evitar StorageLevel que não seja MEMORY_ONLY (a menos que necessário)\n",
    "# o padrão é MEMORY_AND_DISK\n",
    "eventsAggregated = spark.sql(\"\"\"\n",
    "    SELECT user_id, \n",
    "           device_id, \n",
    "           COUNT(1) as event_counts, \n",
    "           COLLECT_LIST(DISTINCT host) as host_array\n",
    "    FROM events\n",
    "    GROUP BY user_id, device_id\n",
    "\"\"\").cache()\n",
    "\n",
    "# se eu precisar usar DISK_ONLY como storage, provavelmente é melhor criar uma tabela de staging !!\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bootcamp.events_aggregated_staging (\n",
    "        user_id BIGINT,\n",
    "        device_id BIGINT,\n",
    "        event_counts BIGINT,\n",
    "        host_array ARRAY<STRING>\n",
    "    )\n",
    "    PARTITIONED BY (ds STRING)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65076f21-c968-486c-bc15-c7ecb21147b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+--------------------+\n",
      "|    user_id| device_id|event_counts|          host_array|\n",
      "+-----------+----------+------------+--------------------+\n",
      "|-2147421007|-807271869|           1|[admin.zachwilson...|\n",
      "|-2147340867|1324700293|           1|  [www.eczachly.com]|\n",
      "|-2147051672| 583904608|           1|  [www.eczachly.com]|\n",
      "+-----------+----------+------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "eventsAggregated.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "715c6d48-e13e-40a1-a9ad-80c24d611901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141935"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eventsAggregated.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "342b93ee-a201-45c6-a4bc-20b903b5b217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- ObjectHashAggregate(keys=[user_id#17], functions=[max(event_counts#61L), collect_list(event_device_id#682, 0, 0)])\n",
      "   +- ObjectHashAggregate(keys=[user_id#17], functions=[partial_max(event_counts#61L), partial_collect_list(event_device_id#682, 0, 0)])\n",
      "      +- Project [user_id#17, event_device_id#682, event_counts#61L]\n",
      "         +- SortMergeJoin [user_id#17], [user_id#687], Inner\n",
      "            :- Sort [user_id#17 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(user_id#17, 4), ENSURE_REQUIREMENTS, [plan_id=457]\n",
      "            :     +- Filter isnotnull(user_id#17)\n",
      "            :        +- FileScan csv [user_id#17] Batched: false, DataFilters: [isnotnull(user_id#17)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/events.csv], PartitionFilters: [], PushedFilters: [IsNotNull(user_id)], ReadSchema: struct<user_id:int>\n",
      "            +- Sort [user_id#687 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(user_id#687, 4), ENSURE_REQUIREMENTS, [plan_id=458]\n",
      "                  +- Project [user_id#687, device_id#688 AS event_device_id#682, event_counts#61L]\n",
      "                     +- Filter isnotnull(user_id#687)\n",
      "                        +- InMemoryTableScan [device_id#688, event_counts#61L, user_id#687], [isnotnull(user_id#687)]\n",
      "                              +- InMemoryRelation [user_id#687, device_id#688, event_counts#61L, host_array#62], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                    +- AdaptiveSparkPlan isFinalPlan=true\n",
      "                                       +- == Final Plan ==\n",
      "                                          ObjectHashAggregate(keys=[user_id#17, device_id#18], functions=[count(1), collect_list(distinct host#20, 0, 0)])\n",
      "                                          +- ShuffleQueryStage 1\n",
      "                                             +- Exchange hashpartitioning(user_id#17, device_id#18, 4), ENSURE_REQUIREMENTS, [plan_id=129]\n",
      "                                                +- ObjectHashAggregate(keys=[user_id#17, device_id#18], functions=[merge_count(1), partial_collect_list(distinct host#20, 0, 0)])\n",
      "                                                   +- *(2) HashAggregate(keys=[user_id#17, device_id#18, host#20], functions=[merge_count(1)])\n",
      "                                                      +- AQEShuffleRead coalesced\n",
      "                                                         +- ShuffleQueryStage 0\n",
      "                                                            +- Exchange hashpartitioning(user_id#17, device_id#18, host#20, 4), ENSURE_REQUIREMENTS, [plan_id=100]\n",
      "                                                               +- *(1) HashAggregate(keys=[user_id#17, device_id#18, host#20], functions=[partial_count(1)])\n",
      "                                                                  +- *(1) Filter isnotnull(user_id#17)\n",
      "                                                                     +- FileScan csv [user_id#17,device_id#18,host#20] Batched: false, DataFilters: [isnotnull(user_id#17)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/events.csv], PartitionFilters: [], PushedFilters: [IsNotNull(user_id)], ReadSchema: struct<user_id:int,device_id:int,host:string>\n",
      "                                       +- == Initial Plan ==\n",
      "                                          ObjectHashAggregate(keys=[user_id#17, device_id#18], functions=[count(1), collect_list(distinct host#20, 0, 0)])\n",
      "                                          +- Exchange hashpartitioning(user_id#17, device_id#18, 4), ENSURE_REQUIREMENTS, [plan_id=66]\n",
      "                                             +- ObjectHashAggregate(keys=[user_id#17, device_id#18], functions=[merge_count(1), partial_collect_list(distinct host#20, 0, 0)])\n",
      "                                                +- HashAggregate(keys=[user_id#17, device_id#18, host#20], functions=[merge_count(1)])\n",
      "                                                   +- Exchange hashpartitioning(user_id#17, device_id#18, host#20, 4), ENSURE_REQUIREMENTS, [plan_id=62]\n",
      "                                                      +- HashAggregate(keys=[user_id#17, device_id#18, host#20], functions=[partial_count(1)])\n",
      "                                                         +- Filter isnotnull(user_id#17)\n",
      "                                                            +- FileScan csv [user_id#17,device_id#18,host#20] Batched: false, DataFilters: [isnotnull(user_id#17)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/events.csv], PartitionFilters: [], PushedFilters: [IsNotNull(user_id)], ReadSchema: struct<user_id:int,device_id:int,host:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join do dataframe lido com a view temporária (em cache)\n",
    "# o único benefício de datasets (como temp views) em cache é caso eu precise usar mais de uma vez\n",
    "\n",
    "# só para tirar um erro de ambiguidade\n",
    "eventsRenamed = eventsAggregated.withColumnRenamed(\"device_id\", \"event_device_id\")\n",
    "\n",
    "usersAndDevices = users.join(eventsRenamed, on=\"user_id\") \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .agg(max(\"event_counts\").alias(\"total_hits\"),\n",
    "         collect_list(\"event_device_id\").alias(\"devices\") )\n",
    "\n",
    "# notar que sse join não é nada escalável...\n",
    "usersAndDevices.explain()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46e2b72f-3a0c-4fa3-80ad-ef228d09097e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- ObjectHashAggregate(keys=[user_id#17], functions=[max(event_counts#61L), collect_list(event_device_id#682, 0, 0)])\n",
      "   +- Exchange hashpartitioning(user_id#17, 4), ENSURE_REQUIREMENTS, [plan_id=505]\n",
      "      +- ObjectHashAggregate(keys=[user_id#17], functions=[partial_max(event_counts#61L), partial_collect_list(event_device_id#682, 0, 0)])\n",
      "         +- Project [user_id#17, event_device_id#682, event_counts#61L]\n",
      "            +- BroadcastHashJoin [user_id#17], [user_id#783], Inner, BuildRight, false\n",
      "               :- Filter isnotnull(user_id#17)\n",
      "               :  +- FileScan csv [user_id#17] Batched: false, DataFilters: [isnotnull(user_id#17)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/events.csv], PartitionFilters: [], PushedFilters: [IsNotNull(user_id)], ReadSchema: struct<user_id:int>\n",
      "               +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=500]\n",
      "                  +- Project [user_id#783, device_id#784 AS event_device_id#682, event_counts#61L]\n",
      "                     +- Filter isnotnull(user_id#783)\n",
      "                        +- InMemoryTableScan [device_id#784, event_counts#61L, user_id#783], [isnotnull(user_id#783)]\n",
      "                              +- InMemoryRelation [user_id#783, device_id#784, event_counts#61L, host_array#62], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                    +- AdaptiveSparkPlan isFinalPlan=true\n",
      "                                       +- == Final Plan ==\n",
      "                                          ObjectHashAggregate(keys=[user_id#17, device_id#18], functions=[count(1), collect_list(distinct host#20, 0, 0)])\n",
      "                                          +- ShuffleQueryStage 1\n",
      "                                             +- Exchange hashpartitioning(user_id#17, device_id#18, 4), ENSURE_REQUIREMENTS, [plan_id=129]\n",
      "                                                +- ObjectHashAggregate(keys=[user_id#17, device_id#18], functions=[merge_count(1), partial_collect_list(distinct host#20, 0, 0)])\n",
      "                                                   +- *(2) HashAggregate(keys=[user_id#17, device_id#18, host#20], functions=[merge_count(1)])\n",
      "                                                      +- AQEShuffleRead coalesced\n",
      "                                                         +- ShuffleQueryStage 0\n",
      "                                                            +- Exchange hashpartitioning(user_id#17, device_id#18, host#20, 4), ENSURE_REQUIREMENTS, [plan_id=100]\n",
      "                                                               +- *(1) HashAggregate(keys=[user_id#17, device_id#18, host#20], functions=[partial_count(1)])\n",
      "                                                                  +- *(1) Filter isnotnull(user_id#17)\n",
      "                                                                     +- FileScan csv [user_id#17,device_id#18,host#20] Batched: false, DataFilters: [isnotnull(user_id#17)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/events.csv], PartitionFilters: [], PushedFilters: [IsNotNull(user_id)], ReadSchema: struct<user_id:int,device_id:int,host:string>\n",
      "                                       +- == Initial Plan ==\n",
      "                                          ObjectHashAggregate(keys=[user_id#17, device_id#18], functions=[count(1), collect_list(distinct host#20, 0, 0)])\n",
      "                                          +- Exchange hashpartitioning(user_id#17, device_id#18, 4), ENSURE_REQUIREMENTS, [plan_id=66]\n",
      "                                             +- ObjectHashAggregate(keys=[user_id#17, device_id#18], functions=[merge_count(1), partial_collect_list(distinct host#20, 0, 0)])\n",
      "                                                +- HashAggregate(keys=[user_id#17, device_id#18, host#20], functions=[merge_count(1)])\n",
      "                                                   +- Exchange hashpartitioning(user_id#17, device_id#18, host#20, 4), ENSURE_REQUIREMENTS, [plan_id=62]\n",
      "                                                      +- HashAggregate(keys=[user_id#17, device_id#18, host#20], functions=[partial_count(1)])\n",
      "                                                         +- Filter isnotnull(user_id#17)\n",
      "                                                            +- FileScan csv [user_id#17,device_id#18,host#20] Batched: false, DataFilters: [isnotnull(user_id#17)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/events.csv], PartitionFilters: [], PushedFilters: [IsNotNull(user_id)], ReadSchema: struct<user_id:int,device_id:int,host:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testando o plano de execução com broadcast join\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "usersAndDevices_broadcast = users.join(broadcast(eventsRenamed), on=\"user_id\") \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .agg(\n",
    "        max(\"event_counts\").alias(\"total_hits\"),\n",
    "        collect_list(\"event_device_id\").alias(\"devices\")\n",
    "    )\n",
    "\n",
    "usersAndDevices_broadcast.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c634bfcf-060a-4c0b-8425-e6b3cf4f3b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+\n",
      "|    user_id|total_hits|             devices|\n",
      "+-----------+----------+--------------------+\n",
      "|-2147470439|         3|[378988111, 37898...|\n",
      "|-2147421007|         1|        [-807271869]|\n",
      "|-2147326548|         1|         [532630305]|\n",
      "+-----------+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usersAndDevices.show(3)\n",
    "type(usersAndDevices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fffbdba1-801e-4ff6-8e71-846064660f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take retorna uma lista de Rows\n",
    "usersAndDevices.take(1)\n",
    "type(usersAndDevices.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c7bbd41-ead9-4c23-b7d4-1f46a64b7600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, device_id: int, event_counts: bigint, host_array: array<string>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eventsAggregated.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35186935-f9c4-4771-9e46-54ff8a0a668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74198947-e3b3-447f-8471-bca178e33a91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
